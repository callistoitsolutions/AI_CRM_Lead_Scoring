{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86938113-d826-49c5-b4e6-f5ccca11b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\HP\\Desktop\\data science\\ML Project Rental New\n",
      "\n",
      "Dataset shape: (5000, 10)\n",
      "\n",
      "Columns: ['lead_id', 'name', 'phone', 'source', 'budget_min', 'budget_max', 'preferred_area', 'bhk', 'user_type', 'move_in_time']\n",
      "\n",
      "First 5 rows:\n",
      "   lead_id    name       phone     source  budget_min  budget_max  \\\n",
      "0        1  Ritesh  9621539141   Facebook       10000       18000   \n",
      "1        2    Amit  9557465220  Instagram       20000       20000   \n",
      "2        3  Anjali  9050891432  Instagram        7000       12000   \n",
      "3        4   Priya  9124138450   Facebook        8000       25000   \n",
      "4        5  Ayesha  9259367346  Instagram       20000       15000   \n",
      "\n",
      "  preferred_area  bhk      user_type move_in_time  \n",
      "0        Andheri    3         Family      1 Month  \n",
      "1          Malad    3  Company Guest     2 Months  \n",
      "2       Madhapur    2         Family      1 Month  \n",
      "3          Thane    3       Bachelor     2 Months  \n",
      "4        Kharadi    2  Company Guest     2 Months  \n",
      "\n",
      "Feature columns: ['budget_mid_norm', 'budget_range_norm', 'budget_flexibility', 'area_popularity', 'user_type_score', 'bhk_score', 'move_in_urgency', 'source_quality', 'lead_quality_score', 'source', 'preferred_area', 'user_type', 'area_tier']\n",
      "\n",
      "Target distribution:\n",
      "is_high_quality\n",
      "0    2922\n",
      "1    2078\n",
      "Name: count, dtype: int64\n",
      "High-quality leads: 41.56%\n",
      "\n",
      "Numerical columns: ['budget_mid_norm', 'budget_range_norm', 'budget_flexibility', 'area_popularity', 'user_type_score', 'bhk_score', 'move_in_urgency', 'source_quality', 'lead_quality_score']\n",
      "Categorical columns: ['source', 'preferred_area', 'user_type', 'area_tier']\n",
      "\n",
      "Training set size: 3750\n",
      "Test set size: 1250\n",
      "Training target distribution: is_high_quality\n",
      "0    0.584267\n",
      "1    0.415733\n",
      "Name: proportion, dtype: float64\n",
      "Test target distribution: is_high_quality\n",
      "0    0.5848\n",
      "1    0.4152\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "RandomForest Evaluation:\n",
      "Accuracy: 1.0000\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       731\n",
      "           1       1.00      1.00      1.00       519\n",
      "\n",
      "    accuracy                           1.00      1250\n",
      "   macro avg       1.00      1.00      1.00      1250\n",
      "weighted avg       1.00      1.00      1.00      1250\n",
      "\n",
      "\n",
      "XGBoost Evaluation:\n",
      "Accuracy: 1.0000\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       731\n",
      "           1       1.00      1.00      1.00       519\n",
      "\n",
      "    accuracy                           1.00      1250\n",
      "   macro avg       1.00      1.00      1.00      1250\n",
      "weighted avg       1.00      1.00      1.00      1250\n",
      "\n",
      "\n",
      "Top 10 Important Features (RandomForest):\n",
      "                            feature  importance\n",
      "8                                x8    0.183888\n",
      "3                                x3    0.145819\n",
      "6                                x6    0.117633\n",
      "0                                x0    0.111500\n",
      "7                                x7    0.109991\n",
      "32          user_type_Company Guest    0.046018\n",
      "34  user_type_Working Professionals    0.042842\n",
      "4                                x4    0.028691\n",
      "31               user_type_Bachelor    0.022139\n",
      "33                 user_type_Family    0.019052\n",
      "\n",
      "Top 10 Important Features (XGBoost):\n",
      "                            feature  importance\n",
      "8                                x8    0.351102\n",
      "34  user_type_Working Professionals    0.112605\n",
      "32          user_type_Company Guest    0.107232\n",
      "7                                x7    0.104767\n",
      "3                                x3    0.088992\n",
      "6                                x6    0.076685\n",
      "0                                x0    0.075259\n",
      "4                                x4    0.057521\n",
      "5                                x5    0.015579\n",
      "1                                x1    0.005715\n",
      "\n",
      "Best model: RandomForest (AUC: 1.0000)\n",
      "\n",
      "Lead Score Distribution:\n",
      "lead_category\n",
      "Cold    2766\n",
      "Hot     2039\n",
      "Warm      49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 Leads by Score:\n",
      "      lead_id    name preferred_area  budget_mid  lead_score lead_category\n",
      "4873     4874  Anjali          Wakad     18500.0       100.0           Hot\n",
      "3235     3236    Amit      Hinjewadi     19000.0       100.0           Hot\n",
      "4943     4944  Anjali          Thane     19000.0       100.0           Hot\n",
      "4549     4550   Komal     Kukatpally     19000.0       100.0           Hot\n",
      "298       299   Pooja     Whitefield     19000.0       100.0           Hot\n",
      "4541     4542   Pooja     Kukatpally     20000.0       100.0           Hot\n",
      "1118     1119  Anjali     Kukatpally     25000.0       100.0           Hot\n",
      "600       601  Ayesha     Kukatpally     21000.0       100.0           Hot\n",
      "2948     2949   Meena     Whitefield     21000.0       100.0           Hot\n",
      "241       242  Anjali     Whitefield     20000.0       100.0           Hot\n",
      "2068     2069   Rahul          Wakad     20000.0       100.0           Hot\n",
      "217       218   Deepa         Bandra     20000.0       100.0           Hot\n",
      "3022     3023   Pooja      Hinjewadi     19000.0       100.0           Hot\n",
      "2650     2651   Mohit      Hinjewadi     19000.0       100.0           Hot\n",
      "3387     3388   Deepa      Hinjewadi     21000.0       100.0           Hot\n",
      "2321     2322   Meena      Hinjewadi     25000.0       100.0           Hot\n",
      "1072     1073  Ritesh     Whitefield     20000.0       100.0           Hot\n",
      "1075     1076   Komal     Kukatpally     17500.0       100.0           Hot\n",
      "930       931   Sneha         Bandra     20000.0       100.0           Hot\n",
      "3015     3016  Ritesh      Hinjewadi     19000.0        99.9           Hot\n",
      "\n",
      "Results saved to: lead_scoring_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. File Inspection\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "df = pd.read_excel('5000_rental_crm_leads.xlsx')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Feature Engineering - Creating meaningful features from existing columns\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create meaningful features from the existing dataset columns\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Budget Features\n",
    "    # Budget range (difference between max and min)\n",
    "    df['budget_range'] = df['budget_max'] - df['budget_min']\n",
    "    \n",
    "    # Budget midpoint (average budget)\n",
    "    df['budget_mid'] = (df['budget_min'] + df['budget_max']) / 2\n",
    "    \n",
    "    # Budget flexibility (ratio of range to midpoint)\n",
    "    df['budget_flexibility'] = df['budget_range'] / (df['budget_mid'] + 1)\n",
    "    \n",
    "    # Normalize budget features (0-1 scale)\n",
    "    df['budget_mid_norm'] = (df['budget_mid'] - df['budget_mid'].min()) / (df['budget_mid'].max() - df['budget_mid'].min())\n",
    "    df['budget_range_norm'] = (df['budget_range'] - df['budget_range'].min()) / (df['budget_range'].max() - df['budget_range'].min())\n",
    "    \n",
    "    # 2. Area Features\n",
    "    # Area popularity score (frequency-based)\n",
    "    area_freq = df['preferred_area'].value_counts(normalize=True)\n",
    "    df['area_popularity'] = df['preferred_area'].map(area_freq)\n",
    "    \n",
    "    # Area tier (based on popularity)\n",
    "    df['area_tier'] = pd.cut(df['area_popularity'], \n",
    "                            bins=[0, 0.02, 0.04, 1.0], \n",
    "                            labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # 3. User Type Features\n",
    "    # User type encoding (one-hot style with numerical values)\n",
    "    user_type_mapping = {\n",
    "        'Family': 3,\n",
    "        'Working Professionals': 2,\n",
    "        'Bachelor': 1,\n",
    "        'Company Guest': 2\n",
    "    }\n",
    "    df['user_type_score'] = df['user_type'].map(user_type_mapping).fillna(1)\n",
    "    \n",
    "    # 4. BHK Features\n",
    "    # BHK preference score (higher BHK = higher score)\n",
    "    df['bhk_score'] = df['bhk'] / 3.0  # Normalize by max BHK\n",
    "    \n",
    "    # 5. Move-in Time Features\n",
    "    # Urgency score based on move-in time\n",
    "    move_in_mapping = {\n",
    "        'Immediate': 4,\n",
    "        'Within 15 Days': 3,\n",
    "        '1 Month': 2,\n",
    "        '2 Months': 1\n",
    "    }\n",
    "    df['move_in_urgency'] = df['move_in_time'].map(move_in_mapping).fillna(1)\n",
    "    \n",
    "    # 6. Source Features\n",
    "    # Source quality score (based on typical conversion rates)\n",
    "    source_mapping = {\n",
    "        'Referral': 4,\n",
    "        'Website': 3,\n",
    "        'WhatsApp': 3,\n",
    "        'Facebook': 2,\n",
    "        'Instagram': 2,\n",
    "        'Google Ads': 1\n",
    "    }\n",
    "    df['source_quality'] = df['source'].map(source_mapping).fillna(1)\n",
    "    \n",
    "    # 7. Composite Features\n",
    "    # Lead Quality Score (weighted combination of features)\n",
    "    df['lead_quality_score'] = (\n",
    "        0.3 * df['budget_mid_norm'] +\n",
    "        0.2 * df['area_popularity'] +\n",
    "        0.2 * df['user_type_score'] / 3.0 +\n",
    "        0.1 * df['bhk_score'] +\n",
    "        0.1 * df['move_in_urgency'] / 4.0 +\n",
    "        0.1 * df['source_quality'] / 4.0\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_featured = create_features(df)\n",
    "\n",
    "# 3. Create Target Variable (y)\n",
    "# Since there's no conversion data, create a meaningful target based on lead quality\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"Create a target variable based on lead quality indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define high-quality lead criteria\n",
    "    high_budget = df['budget_mid'] > df['budget_mid'].quantile(0.6)\n",
    "    popular_area = df['area_popularity'] > df['area_popularity'].quantile(0.6)\n",
    "    urgent_move = df['move_in_urgency'] >= 3\n",
    "    good_source = df['source_quality'] >= 3\n",
    "    family_or_working = df['user_type'].isin(['Family', 'Working Professionals'])\n",
    "    \n",
    "    # Create target: 1 for high-quality leads, 0 for others\n",
    "    df['is_high_quality'] = (\n",
    "        (high_budget.astype(int) + \n",
    "         popular_area.astype(int) + \n",
    "         urgent_move.astype(int) + \n",
    "         good_source.astype(int) + \n",
    "         family_or_working.astype(int)) >= 3\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_featured = create_target_variable(df_featured)\n",
    "\n",
    "# 4. Prepare X and y\n",
    "\n",
    "# Select features for the model\n",
    "feature_cols = [\n",
    "    'budget_mid_norm',\n",
    "    'budget_range_norm',\n",
    "    'budget_flexibility',\n",
    "    'area_popularity',\n",
    "    'user_type_score',\n",
    "    'bhk_score',\n",
    "    'move_in_urgency',\n",
    "    'source_quality',\n",
    "    'lead_quality_score'\n",
    "]\n",
    "\n",
    "# Add categorical features\n",
    "categorical_features = ['source', 'preferred_area', 'user_type', 'area_tier']\n",
    "feature_cols.extend(categorical_features)\n",
    "\n",
    "# Create X and y\n",
    "X = df_featured[feature_cols].copy()\n",
    "y = df_featured['is_high_quality'].copy()\n",
    "\n",
    "print(\"\\nFeature columns:\", feature_cols)\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"High-quality leads: {y.mean():.2%}\")\n",
    "\n",
    "# 5. Preprocessing\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training target distribution: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "# 7. Train Models\n",
    "\n",
    "# 7.1 RandomForest\n",
    "rf_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 7.2 XGBoost\n",
    "xgb_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluate Models\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return y_pred, y_proba\n",
    "\n",
    "# Evaluate both models\n",
    "rf_pred, rf_proba = evaluate_model(rf_model, X_test, y_test, \"RandomForest\")\n",
    "xgb_pred, xgb_proba = evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# 9. Feature Importance\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"Get feature importance from the model\"\"\"\n",
    "    if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    preprocessor = model.named_steps['preprocessor']\n",
    "    \n",
    "    # Get numerical feature names\n",
    "    num_features = preprocessor.named_transformers_['num'].named_steps['scaler'].get_feature_names_out()\n",
    "    \n",
    "    # Get categorical feature names\n",
    "    cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    all_features = np.concatenate([num_features, cat_features])\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Get feature importances\n",
    "rf_importance = get_feature_importance(rf_model, feature_cols)\n",
    "xgb_importance = get_feature_importance(xgb_model, feature_cols)\n",
    "\n",
    "print(\"\\nTop 10 Important Features (RandomForest):\")\n",
    "print(rf_importance.head(10))\n",
    "\n",
    "print(\"\\nTop 10 Important Features (XGBoost):\")\n",
    "print(xgb_importance.head(10))\n",
    "\n",
    "# 10. Generate Lead Scores\n",
    "\n",
    "# Select the better model based on AUC\n",
    "rf_auc = roc_auc_score(y_test, rf_proba)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "\n",
    "if xgb_auc > rf_auc:\n",
    "    best_model = xgb_model\n",
    "    best_model_name = \"XGBoost\"\n",
    "    best_auc = xgb_auc\n",
    "else:\n",
    "    best_model = rf_model\n",
    "    best_model_name = \"RandomForest\"\n",
    "    best_auc = rf_auc\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
    "\n",
    "# Generate lead scores for all data\n",
    "all_scores = best_model.predict_proba(X)[:, 1]\n",
    "df_featured['lead_score'] = (all_scores * 100).round(1)\n",
    "\n",
    "# Create lead categories\n",
    "df_featured['lead_category'] = pd.cut(\n",
    "    df_featured['lead_score'],\n",
    "    bins=[0, 40, 70, 100],\n",
    "    labels=['Cold', 'Warm', 'Hot']\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLead Score Distribution:\")\n",
    "print(df_featured['lead_category'].value_counts())\n",
    "\n",
    "print(\"\\nTop 20 Leads by Score:\")\n",
    "top_leads = df_featured.sort_values('lead_score', ascending=False).head(20)\n",
    "print(top_leads[['lead_id', 'name', 'preferred_area', 'budget_mid', 'lead_score', 'lead_category']])\n",
    "\n",
    "# 11. Save Results\n",
    "output_file = 'lead_scoring_results.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    # Save all scored leads\n",
    "    df_featured.to_excel(writer, sheet_name='All_Leads_Scored', index=False)\n",
    "    \n",
    "    # Save top leads\n",
    "    top_leads.to_excel(writer, sheet_name='Top_20_Leads', index=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    rf_importance.to_excel(writer, sheet_name='RF_Feature_Importance', index=False)\n",
    "    xgb_importance.to_excel(writer, sheet_name='XGB_Feature_Importance', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f55ca2-6c24-42fc-9b54-60988b70f317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
